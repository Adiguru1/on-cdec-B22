Docker > container > trust > manually > Docker-swarm > to manage containers automatically

Google > Borg > 2015 > Linux Foundation > Kubernetes(k8s) -> open source, very advance,
high secure, free to use

              KUBERNETES [container orchestration tool]
      cluster
      nodes{ Master node, Worker nodes}
      pods  (smallest unit of deployment in k8s)

  Master node components
 1) API server > Route the request
 2) ETCD  > Store the data here
 3) Scheduler > Schedule the nodes(server)
 4) Controller Manager > keep the resources in your desired state
                          (current state = Desired state)

  Worker node components
 1) Kubelette > create the pod and monitor it
 2) container runtime > provide environment to create container inside pod
 3) Kube-proxy > provide the networking (ip-addresses) to the pod

   
Pod > smallest unit of deployment in k8s
    > wrapper in which we create containers 
    > we can create multiple containers in one pod 
    > pod has its own ip address



Objects -> Cluster
(RESOURCES) > Node
         > Pod 
         > Namespace
         > service - to expose pod outside and inside  the cluster 
         > Replica set {Earlier -Replication controller}
         > Deployment
         > StatefulSet
         > Daemonset
         > Ingress 
         > Horizontal Pod Autoscalling
         > config map  and secrets variables 


Cluster - on cloud         , on premises,     on server,        webapplcation
          EKS {managed}       Kubeadm          minikube         killercoda {two node cluster}
          AKS                 Kind            {single node}
          GKE                {Self-Managed}



# kubectl {prefix of k8s commands}
# kubectl run <podname> --image=<imagename>   {to create and run the pod}
# kubectl get <resource-name>
# kubectl get node 
# kubectl get pod -o wide  {described details}


* In a pod there are two container one is main and other is side car container

manifest {yaml file}   "Indentation  maintain"
 
vi pod.yaml

apiVersion: "library"
kind:    "Type of resource"
metadata: "data of data"
spec: "service of pod"




apiVersion:  v1   
kind :  Pod       
metadata:  
  name: demopod
  label:
    team: prod
spec:
  containers:
  - name: container1
    image: nginx
  - name: container2
    image: tomcat 
     

after creation of file please apply it

# kubectl apply -f <filename>



*Port no. of container is imp. for communicate with particular container in a pod
*No two container in same pod can have same port
  
        
-->   Intra-pod communication
1) same pod containers connect
2) Use podIP or localhost to connect

#  kubectl exec -it demo-pod -- bash
#  kubectl exec -it -c container2 demo-pod -- bash

-->  Inter-Pod
 
two different pod communication

#kubectl api-resources 
  
Pod get terminate again and again and ip got changed but not label of pod

@@@@@Service@@@@
 type- 1) ClusterIP -  which i used to provide static IP and common/single Ip to the pods, but work only inside         	  (default one) 	        cluster
       
       2) NodePort   -  service resource will generate a random port no.{30k}, and use it with nodeIP       			[nodeIp:nodePort]
       3) LoadBalancer - 

{

apiVersion: v1
kind: Service
metadata: 
  name: my-svc
spec:
  selector:
    team: dev
  ports:
  - port: 80
    targetPort: 80
  type: NodePort

}   


Replication Controller - to keep maintain the no. of pods you create in the running state at a given time

apiVersion: v1
kind: ReplicationController
metadata:
  name: my-rc
  labels:
    app: my-app
spec:
  template:
    metadata:
      name: Pod1
      labels:
        team: test
    spec:
      containers:
      - name: cont1
        image: nginx
  selector:
    team: test
  replicas: 4


*labels are important here
* recreates pod automatically at desired replica
* to delete pod , need to delete rc


Replica-set =>  this work as a set based selector


---> set based labels selector
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: my-rs
  labels:
    app: my-app
spec:
  template:
    metadata:
      name: Pod1
      labels:
        team: test
        team: prod
        group: dev
    spec:
      containers:
      - name: cont1
        image: nginx
  selector:
    matchExpressions:
      key: ["team","group"]
      values: ["test","dev","prod"]
  replicas: 4

--->
no set based selector

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: my-rs
  labels:
    app: my-app
spec:
  template:
    metadata:
      name: Pod1
      labels:
        team: dev       
    spec:
      containers:
      - name: cont1
        image: nginx
  selector:
    matchLabels:
      team: dev
  replicas: 4



@@@@Namespace@@@@@


                   

                      @@@Deployment@@@@
> To manage and maintain that no. of pods are kept running on a particular given time.
>  rolling update and rolling back policy{task}"
****{generally use for stateless application like frontend and backend}
    
$Rolling update > in this policy the resource pods get automatically update as per updation in manifest
>>>>>>strategy -> 1) Rolling Update strategy -  first create then destroy  {defaut}
                  2) Recreate   -  first delete all in one go and then create in one go
                  3) Canary deployment   (task)
                  4) Blue-green deployment (task)



            @@@StatefulSet@@@
 game > level 10 > next day > level 0   --- > stateless
 game > level 10 > next day  > level 10    ----> stateful (previous states matter)

 *** it is use for stateful application like database application 
   ** all pods created by ss have there unique identity start from 0----
 


     @@@configMap and secrets@@@@@
   variables - prevent us to store hardcoded values for the pods in manifest


